{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1a420b",
   "metadata": {},
   "source": [
    "# ðŸ”Š Speaker Recognition using Classical Machine Learning\n",
    "**Project:** VocalPrint â€“ Who's Speaking?\n",
    "\n",
    "This notebook implements speaker identification using classical machine learning algorithms with MFCC features extracted from Mozilla Common Voice data.\n",
    "\n",
    "We will:\n",
    "- Load a subset of Common Voice\n",
    "- Select a few speakers\n",
    "- Extract MFCC features using Librosa\n",
    "- Train and evaluate models (e.g., KNN, SVM)\n",
    "- Visualize performance using a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load 1% subset of the English Common Voice dataset\n",
    "dataset = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"train[:1%]\")\n",
    "dataset = dataset.filter(lambda x: x['audio'] is not None and x['client_id'] is not None)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} examples.\")\n",
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a0f39",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 1: Load Audio Metadata\n",
    "\n",
    "We read the `validated.tsv` file to extract speaker IDs and audio paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe09440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = \"data/cv-corpus-13.0-2023-03-09/en/validated.tsv\"\n",
    "base_audio_path = \"data/cv-corpus-13.0-2023-03-09/en/clips/\"\n",
    "\n",
    "# Load the TSV into a DataFrame\n",
    "df = pd.read_csv(metadata_path, sep=\"\\t\")\n",
    "\n",
    "# Now it's safe to display a sample\n",
    "df[[\"client_id\", \"path\", \"sentence\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0644d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54077/136102632.py:10: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(file_path, sr=None)  # Load with original sampling rate\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cv-corpus-21.0-delta-2025-03-14/en/clips/common_voice_en_41910500.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLibsndfileError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/librosa/core/audio.py:175\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     y, sr_native = \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m sf.SoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/librosa/core/audio.py:208\u001b[39m, in \u001b[36m__soundfile_load\u001b[39m\u001b[34m(path, offset, duration, dtype)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     context = \u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/soundfile.py:658\u001b[39m, in \u001b[36mSoundFile.__init__\u001b[39m\u001b[34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28mself\u001b[39m._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[32m    657\u001b[39m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m \u001b[38;5;28mself\u001b[39m._file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode).issuperset(\u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n\u001b[32m    660\u001b[39m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/soundfile.py:1216\u001b[39m, in \u001b[36mSoundFile._open\u001b[39m\u001b[34m(self, file, mode_int, closefd)\u001b[39m\n\u001b[32m   1215\u001b[39m     err = _snd.sf_error(file_ptr)\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix=\u001b[33m\"\u001b[39m\u001b[33mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.name))\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode_int == _snd.SFM_WRITE:\n\u001b[32m   1218\u001b[39m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[32m   1219\u001b[39m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[32m   1220\u001b[39m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[31mLibsndfileError\u001b[39m: Error opening 'data/cv-corpus-21.0-delta-2025-03-14/en/clips/common_voice_en_41910500.mp3': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33mdata/cv-corpus-21.0-delta-2025-03-14/en/clips/common_voice_en_41910500.mp3\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# === 2. Load the audio file ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m y, sr = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load with original sampling rate\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded audio with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Sample rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# === 3. Extract MFCC features ===\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/librosa/core/audio.py:183\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib.PurePath)):\n\u001b[32m    180\u001b[39m     warnings.warn(\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[33m\"\u001b[39m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     y, sr_native = \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/decorator.py:235\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    234\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/librosa/util/decorators.py:59\u001b[39m, in \u001b[36mdeprecated.<locals>.__wrapper\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[32m     51\u001b[39m warnings.warn(\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/librosa/core/audio.py:239\u001b[39m, in \u001b[36m__audioread_load\u001b[39m\u001b[34m(path, offset, duration, dtype)\u001b[39m\n\u001b[32m    236\u001b[39m     reader = path\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     reader = \u001b[43maudioread\u001b[49m\u001b[43m.\u001b[49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[32m    242\u001b[39m     sr_native = input_file.samplerate\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/audioread/__init__.py:127\u001b[39m, in \u001b[36maudio_open\u001b[39m\u001b[34m(path, backends)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/VoiceScope/.venv/lib/python3.12/site-packages/audioread/rawread.py:59\u001b[39m, in \u001b[36mRawAudioFile.__init__\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28mself\u001b[39m._fh = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m         \u001b[38;5;28mself\u001b[39m._file = aifc.open(\u001b[38;5;28mself\u001b[39m._fh)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/cv-corpus-21.0-delta-2025-03-14/en/clips/common_voice_en_41910500.mp3'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === 1. Define path to your MP3 file ===\n",
    "file_path = \"data/cv-corpus-21.0-delta-2025-03-14/en/clips/common_voice_en_41910500.mp3\"\n",
    "\n",
    "# === 2. Load the audio file ===\n",
    "y, sr = librosa.load(file_path, sr=None)  # Load with original sampling rate\n",
    "\n",
    "print(f\"Loaded audio with shape: {y.shape}, Sample rate: {sr}\")\n",
    "\n",
    "# === 3. Extract MFCC features ===\n",
    "mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "print(f\"MFCC shape: {mfcc.shape}\")\n",
    "\n",
    "# === 4. Visualize the MFCCs ===\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfcc, x_axis='time', sr=sr)\n",
    "plt.colorbar()\n",
    "plt.title(\"MFCC - Mel Frequency Cepstral Coefficients\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e51ff3",
   "metadata": {},
   "source": [
    "## ðŸ‘¤ Step 2: Filter Speakers\n",
    "\n",
    "We select the top N speakers with at least 10 samples to ensure class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "speaker_counts = Counter(dataset['client_id'])\n",
    "N_SPEAKERS = 5\n",
    "selected_speakers = [s for s, c in speaker_counts.items() if c > 10][:N_SPEAKERS]\n",
    "\n",
    "filtered_dataset = dataset.filter(lambda x: x['client_id'] in selected_speakers)\n",
    "print(f\"Filtered to {len(filtered_dataset)} samples from {N_SPEAKERS} speakers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_mfcc(audio_array, sr, n_mfcc=13):\n",
    "    mfcc = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for example in tqdm(filtered_dataset):\n",
    "    audio = example[\"audio\"][\"array\"]\n",
    "    sr = example[\"audio\"][\"sampling_rate\"]\n",
    "    label = example[\"client_id\"]\n",
    "    \n",
    "    mfcc_feat = extract_mfcc(audio, sr)\n",
    "    X.append(mfcc_feat)\n",
    "    y.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ace73",
   "metadata": {},
   "source": [
    "## ðŸŽ¼ Step 3: Extract MFCC Features\n",
    "\n",
    "We use Librosa to extract 13 MFCC coefficients for each audio sample, averaging across time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a49250",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 4: Train a K-Nearest Neighbors (KNN) Classifier\n",
    "\n",
    "We split the dataset and train a basic KNN classifier to identify speakers based on MFCC features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dede88d",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 5: Confusion Matrix\n",
    "\n",
    "We visualize how well the classifier performs across all speakers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c26e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - KNN Classifier\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ea56a",
   "metadata": {},
   "source": [
    "## âœ… Summary & Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "- Used Mozilla Common Voice to extract voice samples\n",
    "- Focused on 5 speakers for a balanced classification problem\n",
    "- Extracted MFCC features using Librosa\n",
    "- Trained a KNN model and evaluated it with classification metrics and a confusion matrix\n",
    "\n",
    "**Next:**\n",
    "- Try SVM, Logistic Regression, and Random Forest\n",
    "- Tune hyperparameters (e.g., number of MFCCs, K value in KNN)\n",
    "- Compare with CNNs using Mel Spectrograms in the next notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c7a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_path = Path(\"/home/ahmed-bashir/Documents/school/VoiceScope/data/cv-corpus-21.0-delta-2025-03-14-en/cv-corpus-21.0-delta-2025-03-14/en/clips/common_voice_en_41980499.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5f504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "X_train shape: (179, 13)\n",
      "y_train shape: (179,)\n",
      "Unique speakers: 22\n",
      "\n",
      "Test set:\n",
      "X_test shape: (45, 13)\n",
      "y_test shape: (45,)\n",
      "Unique speakers: 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the TSV metadata\n",
    "base_dir = Path.cwd().parent\n",
    "tsv_path = base_dir / \"data\" / \"cv-corpus-21.0-delta-2025-03-14-en\" / \"cv-corpus-21.0-delta-2025-03-14\" / \"en\" / \"validated.tsv\"\n",
    "df = pd.read_csv(tsv_path, sep='\\t')\n",
    "\n",
    "# Load training data\n",
    "train_data = np.load(base_dir / \"audios\" / \"train.npz\")\n",
    "X_train = train_data['X']\n",
    "y_train = train_data['y']\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"Unique speakers:\", len(np.unique(y_train)))\n",
    "\n",
    "# Load test data\n",
    "test_data = np.load(base_dir / \"audios\" / \"test.npz\")\n",
    "X_test = test_data['X']\n",
    "y_test = test_data['y']\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"Unique speakers:\", len(np.unique(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b38acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (179, 13)\n",
      "y_train shape: (179,)\n",
      "X_test shape: (45, 13)\n",
      "y_test shape: (45,)\n",
      "Train speakers: 22\n",
      "Test speakers: 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the file\n",
    "split_data = np.load(base_dir / \"audios\" / \"train_test_split.npz\")\n",
    "\n",
    "# Extract components\n",
    "X_train = split_data['X_train']\n",
    "X_test = split_data['X_test']\n",
    "y_train = split_data['y_train']\n",
    "y_test = split_data['y_test']\n",
    "\n",
    "# Display summary\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "print(\"Train speakers:\", len(np.unique(y_train)))\n",
    "print(\"Test speakers:\", len(np.unique(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af19cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "X_train shape: (179, 13)\n",
      "y_train shape: (179,)\n",
      "Unique speakers: 22\n",
      "\n",
      "Test set:\n",
      "X_test shape: (45, 13)\n",
      "y_test shape: (45,)\n",
      "Unique speakers: 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load training data\n",
    "train_data = np.load(base_dir / \"audios\" / \"train.npz\")\n",
    "X_train = train_data['X']\n",
    "y_train = train_data['y']\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"Unique speakers:\", len(np.unique(y_train)))\n",
    "\n",
    "# Load test data\n",
    "test_data = np.load(base_dir / \"audios\" / \"test.npz\")\n",
    "X_test = test_data['X']\n",
    "y_test = test_data['y']\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"Unique speakers:\", len(np.unique(y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
